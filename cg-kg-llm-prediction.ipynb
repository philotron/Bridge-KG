{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa167f6",
   "metadata": {},
   "source": [
    "# Experiments: Conversational Grounding LLM Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49bcf83",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains the code for the experiments for the paper titled: \"Bridging Information Gaps in Dialogues With Grounded Exchanges Using Knowledge Graphs\" submitted to SIGDIAL 2024. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdbc50f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from deepdiff import DeepDiff\n",
    "import dotenv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d470725a",
   "metadata": {},
   "source": [
    "### Load Dialogue Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dialogue_corpus/annotated_dialogue_corpus.xlsx'\n",
    "df = pd.read_excel(file_path, header=0)\n",
    "df['topic'] = df['topic'].str.lower()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation_dictionary(df):\n",
    "    # Create dictionary to store conversations for each channel-topic combination\n",
    "    df_conv_dict = {}\n",
    "\n",
    "    # Get unique channel numbers and topics\n",
    "    channels = df['room_name'].unique()\n",
    "    topics = df['topic'].unique()\n",
    "\n",
    "    # Iterate through each channel and topic combination\n",
    "    for channel in channels:\n",
    "        for topic in topics:\n",
    "            # Filter dataframe based on channel and topic\n",
    "            filtered_df = df[(df['room_name'] == channel) & (df['topic'] == topic)]\n",
    "            \n",
    "            if not filtered_df.empty:\n",
    "                # Store filtered dataframe in the dictionary\n",
    "                df_conv_dict[(channel, topic)] = filtered_df\n",
    "            else: pass\n",
    "\n",
    "    return df_conv_dict\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your dataframe with the provided columns\n",
    "df_conv_dict = create_conversation_dictionary(df)\n",
    "\n",
    "# Accessing datasets for each channel-topic combination\n",
    "#for (channel, topic), dataset in df_conv_dict.items():\n",
    "#    print(f\"Channel: {channel}, Topic: {topic}\")\n",
    "#    print(dataset)\n",
    "#    print(\"\\n\")\n",
    "len(df_conv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d652dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dialogue into list of input subdialogues for LLM\n",
    "# Create list with indices for the LLM predictions\n",
    "def create_prompt_input_dict(df_conv_dict):\n",
    "    prompt_input_dict = {}\n",
    "    for (channel, topic), df in df_conv_dict.items():\n",
    "        prompt_input_dict[(channel, topic)] = []\n",
    "        label_indices = df[df['grounding_label'].isin([\"explicit\", \"implicit\", \"clarification\"])].index.tolist()\n",
    "        current_dialogue = []\n",
    "        for i in label_indices:\n",
    "            current_dialogue = []\n",
    "            for index, row in df[[\"role\", \"message\", \"grounding_label\", \"grounded_knowledge\"]].iterrows():\n",
    "                role = row['role']\n",
    "                message = row['message']\n",
    "                grounding_label = row['grounding_label']\n",
    "                grounded_knowledge = row['grounded_knowledge']\n",
    "\n",
    "                current_dialogue.append(f\"{role}: {message}\")\n",
    "\n",
    "                if index == i:\n",
    "                    system_knowledge = df[df[\"grounded_knowledge\"] != \"-\"][\"grounded_knowledge\"].iloc[-1]\n",
    "                    prompt_input_dict[(channel, topic)] += [{'label_index': i, 'label_indices': label_indices, 'dialogue': current_dialogue, 'grounding_label': grounding_label, 'grounded_knowledge': grounded_knowledge, 'system_knowledge': system_knowledge}]\n",
    "                    break\n",
    "    return prompt_input_dict\n",
    "\n",
    "prompt_input_dict = create_prompt_input_dict(df_conv_dict)\n",
    "len(prompt_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d450cc",
   "metadata": {},
   "source": [
    "### Define Prompts for LLM in Chat Format\n",
    "\n",
    "* CLS = classification of grounding label\n",
    "* GK = prediction of grounded knowledge subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CLS prompts\n",
    "def get_CLS_system_prompt() -> str:\n",
    "    ''' Returns the system instruction for CLS of grounding labels'''\n",
    "    return \"\"\"Predict the grounding label for the last response in the 'Input Dialogue:'. The label indicates whether the knowledge in the dialogue was accepted. Choose one of the following labels:\n",
    "    explicit: The response confirms understanding or acceptance (e.g., 'okay', 'thanks', 'alright', 'nice') without seeking clarification.\n",
    "    clarification: The response seeks clarification about a previous dialogue snippet.\n",
    "    implicit: The response moves the conversation forward without explicitly confirming or seeking clarification.\"\"\"\n",
    "\n",
    "    #clarification: The response seeks clarification about a previous dialogue snippet.\n",
    "def get_CLS_zero_shot_chat_prompt(input_dialogue: str, system_message: bool = True):\n",
    "    ''' Returns the template for the zero-shot prompt that predicts a grounding label given a dialogue'''\n",
    "    message = []\n",
    "\n",
    "    if system_message:\n",
    "        message.append({\"role\": \"system\", \"content\": f\"{get_CLS_system_prompt()}\"})\n",
    "\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"Input dialogue: {input_dialogue}\n",
    "Output label: \"\"\"})\n",
    "    return message\n",
    "\n",
    "user_CLS_example_1 = \"\"\"\n",
    "Input Dialogue:\n",
    "seeker: Can you give me some information about your dataset?\n",
    "provider: My dataset includes information on buildings of Gothic architecture.\n",
    "seeker: How tall is the Cologne Cathedral?\n",
    "\"\"\"\n",
    "assistant_CLS_example_1 = \"\"\"\n",
    "Output Label: implicit\n",
    "\"\"\"\n",
    "user_CLS_example_2 = \"\"\"\n",
    "Input Dialogue:\n",
    "provider: Monitors have different attributes like size or panel technology.\n",
    "provider: There are some with an aspect ratio of 21:9.\n",
    "seeker: What is aspect ratio?\n",
    "\"\"\"\n",
    "assistant_CLS_example_2 = \"\"\"\n",
    "Output Label: clarification\n",
    "\"\"\"\n",
    "user_CLS_example_3 = \"\"\"\n",
    "Input Dialogue:\n",
    "provider: An elephant's average lifespan is around 65 years.\n",
    "seeker: I see, good to know.\n",
    "\"\"\"\n",
    "assistant_CLS_example_3 = \"\"\"\n",
    "Output Label: explicit\n",
    "\"\"\"\n",
    "\n",
    "def get_CLS_few_shot_chat_prompt(input_dialogue: str, system_message: bool = True) -> list:\n",
    "    ''' Returns the template for the few-shot prompt that predicts a grounding label given a dialogue'''\n",
    "    message = []\n",
    "    # system instruction\n",
    "    if system_message:\n",
    "        message.append({\"role\": \"system\", \"content\": f\"{get_CLS_system_prompt()}\"})\n",
    "    # few-shot examples\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_CLS_example_1}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_CLS_example_1}\"\"\"})\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_CLS_example_2}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_CLS_example_2}\"\"\"})\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_CLS_example_3}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_CLS_example_3}\"\"\"})\n",
    "    # input dialogue\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"Input dialogue: {input_dialogue}\n",
    "Output label: \"\"\"})\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78112b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GK prompts\n",
    "def get_GK_system_prompt() -> str:\n",
    "    ''' Returns the system instruction for prediction of grounded knowledge'''\n",
    "    return '''Your task is to identify the knowledge items that have been grounded by the conversation partners in the 'Input Dialogue'. The items of mutually grounded knowledge must be explicitly mentioned in the dialogue. Based on the complete set of 'System Knowledge', your task is to generate the subset of knowledge items that have been grounded so far. Ensure that the output is a valid JSON-LD structure (an array of JSON objects) and only include knowledge items from the formatted 'System Knowledge'.'''\n",
    "\n",
    "def get_GK_zero_shot_chat_prompt(system_knowledge: str, input_dialogue: str, system_message: bool = True) -> list:\n",
    "    ''' Returns the template for the zero-shot prompt that predicts a JSON with grounded knowledge given a dialogue'''\n",
    "    message = []\n",
    "\n",
    "    if system_message:\n",
    "        message.append({\"role\": \"system\", \"content\": f\"{get_GK_system_prompt()}\"})\n",
    "    \n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"System Knowledge: {system_knowledge}\n",
    "    Input Dialogue: {input_dialogue}\n",
    "    Output JSON-LD: \"\"\"})\n",
    "    return message\n",
    "\n",
    "user_GK_example_1 = \"\"\"\n",
    "System Knowledge: [{\"@context\": [\"http://www.w3.org/ns/csvw\", {\"schema\": \"http://schema.org\"}], \"@id\": \"http://example.org/american-presidents\", \"url\": \"american-presidents.csv\", \"schema:description\": \"The table contains information about American presidents\", \"tableSchema\": {\"columns\": [{\"name\": \"name\", \"datatype\": \"string\"}, {\"name\": \"term\", \"datatype\": \"string\"}, {\"name\": \"party\", \"datatype\": \"string\"}, {\"name\": \"election_year\", \"datatype\": \"integer\"}]}, \"primaryKey\": \"name\"}, {\"@type\": \"schema:Person\", \"name\": \"Barack Obama\", \"party\": \"Democratic\"}]\n",
    "Input Dialogue:\n",
    "seeker: Can you give me an example entry from your dataset?\n",
    "provider: One of the presidents in the list is Barack Obama.\n",
    "seeker: Thanks. What party does he belong to?\n",
    "\"\"\"\n",
    "assistant_GK_example_1 = \"\"\"\n",
    "Output JSON-LD: [{\"@context\": [\"http://www.w3.org/ns/csvw\", {\"schema\": \"http://schema.org\"}], \"@id\": \"http://example.org/american-presidents\", \"url\": \"american-presidents.csv\", \"schema:description\": \"The table contains information about American presidents\", \"tableSchema\": {\"columns\": [{\"name\": \"name\", \"datatype\": \"string\"}]}, \"primaryKey\": \"name\"}, {\"@type\": \"schema:Person\", \"name\": \"Barack Obama\"}]\n",
    "\"\"\"\n",
    "user_GK_example_2 = \"\"\"\n",
    "System Knowledge: [{\"@context\": [\"http://www.w3.org/ns/csvw\", {\"schema\": \"http://schema.org\"}], \"@id\": \"http://example.org/greek-islands\", \"url\": \"greek-islands.csv\", \"schema:description\": \"The table contains information about islands in Greece\", \"tableSchema\": {\"columns\": [{\"name\": \"island\", \"datatype\": \"string\"}, {\"name\": \"area_in_km2\", \"datatype\": \"integer\", \"minimum\": 64, \"maximum\": 8336}, {\"name\": \"cluster\", \"datatype\": \"string\"}]}, \"primaryKey\": \"island\"}, {\"@type\": \"schema:Place\", \"island\": \"Crete\", \"area_in_km2\": 8336, \"cluster\": \"Cretan\"}, {\"@type\": \"schema:Place\", \"island\": \"Alonnisos\", \"area_in_km2\": 64, \"cluster\": \"Sporades\"}, {\"@type\": \"schema:Place\", \"island\": \"Lesbos\", \"area_in_km2\": 1633, \"cluster\": \"North Aegean Islands\"}]\n",
    "Input Dialogue:\n",
    "provider: My dataset contains information on Greek islands. For example, there is Crete with an area of 8336 square kilometers.\n",
    "provider: That makes it the largest island in Greece.\n",
    "seeker: Which one is the smallest and what is its area?\n",
    "\"\"\"\n",
    "assistant_GK_example_2 = \"\"\"\n",
    "Output JSON-LD: [{\"@context\": [\"http://www.w3.org/ns/csvw\", {\"schema\": \"http://schema.org\"}], \"@id\": \"http://example.org/greek-islands\", \"url\": \"greek-islands.csv\", \"schema:description\": \"The table contains information about islands in Greece\", \"tableSchema\": {\"columns\": [{\"name\": \"island\", \"datatype\": \"string\"}, {\"name\": \"area_in_km2\", \"datatype\": \"integer\", \"maximum\": 8336}]}, \"primaryKey\": \"island\"}, {\"@type\": \"schema:Place\", \"island\": \"Crete\", \"area_in_km2\": 8336}]\n",
    "\"\"\"\n",
    "user_GK_example_3 = \"\"\"\n",
    "System Knowledge: [{\"@context\": [\"http://www.w3.org/ns/csvw\", {\"schema\": \"http://schema.org\"}], \"@id\": \"http://example.org/android-smartphones\", \"url\": \"android-smartphones.csv\", \"schema:description\": \"The table contains information about Android smartphones\", \"tableSchema\": {\"columns\": [{\"name\": \"model\", \"datatype\": \"string\"}, {\"name\": \"developer\", \"datatype\": \"string\"}, {\"name\": \"release_year\", \"datatype\": \"integer\", \"minimum\": 2008, \"maximum\": 2024}, {\"name\": \"android_version\", \"datatype\": \"string\"}]}, \"primaryKey\": \"model\"}, {\"@type\": \"schema:Product\", \"model\": \"HTC Dream\", \"developer:\": \"HTC\", \"release_year\": \"2008\"}, {\"@type\": \"schema:Product\", \"model\": \"LG Wing\", \"developer:\": \"LG\", \"release_year\": \"2020\", \"android_version\": \"Android 10\"}, {\"@type\": \"schema:Product\", \"release_year\": \"2024\"}]\n",
    "Input Dialogue:\n",
    "provider: I can provide technical information about Android smartphones.\n",
    "provider: One column contains data about the model and another specifies its release year.\n",
    "seeker: I see, good to know.\n",
    "\"\"\"\n",
    "assistant_GK_example_3 = \"\"\"\n",
    "Output JSON-LD: [{\"@context\": [\"http://www.w3.org/ns/csvw\", {\"schema\": \"http://schema.org\"}], \"@id\": \"http://example.org/android-smartphones\", \"url\": \"android-smartphones.csv\", \"schema:description\": \"The table contains information about Android smartphones\", \"tableSchema\": {\"columns\": [{\"name\": \"model\", \"datatype\": \"string\"}, {\"name\": \"release_year\", \"datatype\": \"integer\"}]}, \"primaryKey\": \"model\"}]\n",
    "\"\"\"\n",
    "\n",
    "def get_GK_few_shot_chat_prompt(system_knowledge: str, input_dialogue: str, system_message: bool = True) -> list:\n",
    "    ''' Returns the template for the few-shot prompt that predicts a JSON with grounded knowledge given a dialogue'''\n",
    "    message = []\n",
    "    # system instruction\n",
    "    if system_message:\n",
    "        message.append({\"role\": \"system\", \"content\": f\"{get_GK_system_prompt()}\"})\n",
    "    # few-shot examples\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_GK_example_1}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_GK_example_1}\"\"\"})\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_GK_example_2}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_GK_example_2}\"\"\"})\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_GK_example_3}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_GK_example_3}\"\"\"})\n",
    "    # input\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"System Knowledge: {system_knowledge}\n",
    "    Input Dialogue: {input_dialogue}\n",
    "    Output JSON-LD: \"\"\"})\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f48382",
   "metadata": {},
   "source": [
    "### Send Prompt to LLM (OpenAI API / Groq API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b49204",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(\".env\", override=True)\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY']) # used for gpt models\n",
    "#client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\")) # used for llama models\n",
    "\n",
    "def query_gpt(prompt): \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\", # set model version = gpt-3.5-turbo-0125 or gpt-4o or llama3-8b-8192 or llama3-70b-8192\n",
    "        max_tokens = 4096,\n",
    "        seed=1,\n",
    "        messages=prompt, # provide prompt in chat format\n",
    "        temperature=0) # set model temperature = 0\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84adb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference to get LLM predictions\n",
    "def run_CLS_inference(df, prompt_input, n_last_turns=None):\n",
    "    df_predictions = df.copy(deep=True)\n",
    "    df_predictions[\"CLS_prediction\"] = \"\"\n",
    "    df_predictions[\"CLS_prompt\"] = \"\"\n",
    "    # n_last_turns = n_last_turns # number of last input turns before last dialogue utterance\n",
    "    label_indices = prompt_input[0][\"label_indices\"] # get label indices\n",
    "    for idx in label_indices:\n",
    "        print(\"label index:\", idx)\n",
    "        if n_last_turns: # provide number of last input turns before last dialogue utterance\n",
    "            context_dialogue = \"\\n\".join(prompt_input[label_indices.index(idx)][\"dialogue\"][-(n_last_turns+1):])\n",
    "        else: # if None: provide full dialogue history\n",
    "            context_dialogue = \"\\n\".join(prompt_input[label_indices.index(idx)][\"dialogue\"])\n",
    "            print(idx, label_indices, label_indices.index(idx))\n",
    "        system_knowledge = prompt_input[label_indices.index(idx)][\"system_knowledge\"]\n",
    "        prompt = get_CLS_few_shot_chat_prompt(context_dialogue, system_knowledge) # <----------------ZERO vs FEW SHOT\n",
    "        print(\"prompt:\", prompt)\n",
    "        response = query_gpt(prompt=prompt)\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        print(\"prediction:\", result)\n",
    "        print(\"\\n\")\n",
    "        df_predictions.at[idx, \"CLS_prompt\"] = prompt\n",
    "        df_predictions.at[idx, \"CLS_prediction\"] = result\n",
    "        time.sleep(0.1)\n",
    "    return df_predictions\n",
    "\n",
    "def merge_dataframes(df_conv_dict):\n",
    "    # concatenate all dataframes in the dictionary\n",
    "    merged_df = pd.concat(df_conv_dict.values(), ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5badc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"CLS_few_all_gpt35\"\n",
    "# Set number of n last dialogue turns\n",
    "n_last_turns = None\n",
    "\n",
    "df_conv_dict[('channel-01','history')] = run_CLS_inference(df_conv_dict[('channel-01','history')], prompt_input_dict[('channel-01','history')], n_last_turns)\n",
    "df_conv_dict[('channel-01','sports')] = run_CLS_inference(df_conv_dict[('channel-01','sports')], prompt_input_dict[('channel-01','sports')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-02','geography')] = run_CLS_inference(df_conv_dict[('channel-02','geography')], prompt_input_dict[('channel-02','geography')], n_last_turns)\n",
    "df_conv_dict[('channel-02','media')] = run_CLS_inference(df_conv_dict[('channel-02','media')], prompt_input_dict[('channel-02','media')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-03','media')] = run_CLS_inference(df_conv_dict[('channel-03','media')], prompt_input_dict[('channel-03','media')], n_last_turns)\n",
    "df_conv_dict[('channel-03','nutrition')] = run_CLS_inference(df_conv_dict[('channel-03','nutrition')], prompt_input_dict[('channel-03','nutrition')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-04','history')] = run_CLS_inference(df_conv_dict[('channel-04','history')], prompt_input_dict[('channel-04','history')], n_last_turns)\n",
    "df_conv_dict[('channel-04','media')] = run_CLS_inference(df_conv_dict[('channel-04','media')], prompt_input_dict[('channel-04','media')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-06','geography')] = run_CLS_inference(df_conv_dict[('channel-06','geography')], prompt_input_dict[('channel-06','geography')], n_last_turns)\n",
    "df_conv_dict[('channel-06','media')] = run_CLS_inference(df_conv_dict[('channel-06','media')], prompt_input_dict[('channel-06','media')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-07','history')] = run_CLS_inference(df_conv_dict[('channel-07','history')], prompt_input_dict[('channel-07','history')], n_last_turns)\n",
    "df_conv_dict[('channel-07','sports')] = run_CLS_inference(df_conv_dict[('channel-07','sports')], prompt_input_dict[('channel-07','sports')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-08','history')] = run_CLS_inference(df_conv_dict[('channel-08','history')], prompt_input_dict[('channel-08','history')], n_last_turns)\n",
    "df_conv_dict[('channel-08','geography')] = run_CLS_inference(df_conv_dict[('channel-08','geography')], prompt_input_dict[('channel-08','geography')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-09','history')] = run_CLS_inference(df_conv_dict[('channel-09','history')], prompt_input_dict[('channel-09','history')], n_last_turns)\n",
    "df_conv_dict[('channel-09','geography')] = run_CLS_inference(df_conv_dict[('channel-09','geography')], prompt_input_dict[('channel-09','geography')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-10','sports')] = run_CLS_inference(df_conv_dict[('channel-10','sports')], prompt_input_dict[('channel-10','sports')], n_last_turns)\n",
    "df_conv_dict[('channel-10','nutrition')] = run_CLS_inference(df_conv_dict[('channel-10','nutrition')], prompt_input_dict[('channel-10','nutrition')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-11','sports')] = run_CLS_inference(df_conv_dict[('channel-11','sports')], prompt_input_dict[('channel-11','sports')], n_last_turns)\n",
    "df_conv_dict[('channel-11','nutrition')] = run_CLS_inference(df_conv_dict[('channel-11','nutrition')], prompt_input_dict[('channel-11','nutrition')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-12','geography')] = run_CLS_inference(df_conv_dict[('channel-12','geography')], prompt_input_dict[('channel-12','geography')], n_last_turns)\n",
    "df_conv_dict[('channel-12','media')] = run_CLS_inference(df_conv_dict[('channel-12','media')], prompt_input_dict[('channel-12','media')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-13','geography')] = run_CLS_inference(df_conv_dict[('channel-13','geography')], prompt_input_dict[('channel-13','geography')], n_last_turns)\n",
    "df_conv_dict[('channel-13','nutrition')] = run_CLS_inference(df_conv_dict[('channel-13','nutrition')], prompt_input_dict[('channel-13','nutrition')], n_last_turns)\n",
    "\n",
    "df_conv_dict[('channel-14','sports')] = run_CLS_inference(df_conv_dict[('channel-14','sports')], prompt_input_dict[('channel-14','sports')], n_last_turns)\n",
    "df_conv_dict[('channel-14','nutrition')] = run_CLS_inference(df_conv_dict[('channel-14','nutrition')], prompt_input_dict[('channel-14','nutrition')], n_last_turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdfa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference to get LLM predictions\n",
    "def run_GK_inference(df, prompt_input):\n",
    "    df_predictions = df.copy(deep=True)\n",
    "    df_predictions[\"GK_prediction\"] = \"\"\n",
    "    df_predictions[\"GK_prompt\"] = \"\"\n",
    "    label_indices = prompt_input[0][\"label_indices\"] # get label indices\n",
    "    for idx in label_indices:\n",
    "        print(\"label index:\", idx)\n",
    "        context_dialogue = \"\\n\".join(prompt_input[label_indices.index(idx)][\"dialogue\"]) # for GK always include full dialogue history\n",
    "        system_knowledge = prompt_input[label_indices.index(idx)][\"system_knowledge\"]\n",
    "        grounding_label = prompt_input[label_indices.index(idx)][\"grounding_label\"]\n",
    "        if grounding_label != \"clarification\":\n",
    "            prompt = get_GK_few_shot_chat_prompt(context_dialogue, system_knowledge) # <--------ZERO vs FEW SHOT\n",
    "            print(\"--> prompt:\", prompt)\n",
    "            response = query_gpt(prompt=prompt)\n",
    "            result = response.choices[0].message.content.strip()\n",
    "            print(\"--> prediction:\", result)\n",
    "            print(\"\\n\")\n",
    "            df_predictions.at[idx, \"GK_prompt\"] = prompt\n",
    "            df_predictions.at[idx, \"GK_prediction\"] = result\n",
    "        else:\n",
    "            print(grounding_label)\n",
    "            print(\"\\n\")\n",
    "        time.sleep(0.01)\n",
    "    return df_predictions\n",
    "\n",
    "def merge_dataframes(df_conv_dict):\n",
    "    # concatenate all dataframes in the dictionary\n",
    "    merged_df = pd.concat(df_conv_dict.values(), ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85266260",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"GK_few_shot_gpt35\"\n",
    "\n",
    "df_conv_dict[('channel-01','history')] = run_GK_inference(df_conv_dict[('channel-01','history')], prompt_input_dict[('channel-01','history')])\n",
    "df_conv_dict[('channel-01','sports')] = run_GK_inference(df_conv_dict[('channel-01','sports')], prompt_input_dict[('channel-01','sports')])\n",
    "\n",
    "df_conv_dict[('channel-02','geography')] = run_GK_inference(df_conv_dict[('channel-02','geography')], prompt_input_dict[('channel-02','geography')])\n",
    "df_conv_dict[('channel-02','media')] = run_GK_inference(df_conv_dict[('channel-02','media')], prompt_input_dict[('channel-02','media')])\n",
    "\n",
    "df_conv_dict[('channel-03','media')] = run_GK_inference(df_conv_dict[('channel-03','media')], prompt_input_dict[('channel-03','media')])\n",
    "df_conv_dict[('channel-03','nutrition')] = run_GK_inference(df_conv_dict[('channel-03','nutrition')], prompt_input_dict[('channel-03','nutrition')])\n",
    "\n",
    "df_conv_dict[('channel-04','history')] = run_GK_inference(df_conv_dict[('channel-04','history')], prompt_input_dict[('channel-04','history')])\n",
    "df_conv_dict[('channel-04','media')] = run_GK_inference(df_conv_dict[('channel-04','media')], prompt_input_dict[('channel-04','media')])\n",
    "\n",
    "df_conv_dict[('channel-06','geography')] = run_GK_inference(df_conv_dict[('channel-06','geography')], prompt_input_dict[('channel-06','geography')])\n",
    "df_conv_dict[('channel-06','media')] = run_GK_inference(df_conv_dict[('channel-06','media')], prompt_input_dict[('channel-06','media')])\n",
    "\n",
    "df_conv_dict[('channel-07','history')] = run_GK_inference(df_conv_dict[('channel-07','history')], prompt_input_dict[('channel-07','history')])\n",
    "df_conv_dict[('channel-07','sports')] = run_GK_inference(df_conv_dict[('channel-07','sports')], prompt_input_dict[('channel-07','sports')])\n",
    "\n",
    "df_conv_dict[('channel-08','history')] = run_GK_inference(df_conv_dict[('channel-08','history')], prompt_input_dict[('channel-08','history')])\n",
    "df_conv_dict[('channel-08','geography')] = run_GK_inference(df_conv_dict[('channel-08','geography')], prompt_input_dict[('channel-08','geography')])\n",
    "\n",
    "df_conv_dict[('channel-09','history')] = run_GK_inference(df_conv_dict[('channel-09','history')], prompt_input_dict[('channel-09','history')])\n",
    "df_conv_dict[('channel-09','geography')] = run_GK_inference(df_conv_dict[('channel-09','geography')], prompt_input_dict[('channel-09','geography')])\n",
    "\n",
    "df_conv_dict[('channel-10','sports')] = run_GK_inference(df_conv_dict[('channel-10','sports')], prompt_input_dict[('channel-10','sports')])\n",
    "df_conv_dict[('channel-10','nutrition')] = run_GK_inference(df_conv_dict[('channel-10','nutrition')], prompt_input_dict[('channel-10','nutrition')])\n",
    "\n",
    "df_conv_dict[('channel-11','sports')] = run_GK_inference(df_conv_dict[('channel-11','sports')], prompt_input_dict[('channel-11','sports')])\n",
    "df_conv_dict[('channel-11','nutrition')] = run_GK_inference(df_conv_dict[('channel-11','nutrition')], prompt_input_dict[('channel-11','nutrition')])\n",
    "\n",
    "df_conv_dict[('channel-12','geography')] = run_GK_inference(df_conv_dict[('channel-12','geography')], prompt_input_dict[('channel-12','geography')])\n",
    "df_conv_dict[('channel-12','media')] = run_GK_inference(df_conv_dict[('channel-12','media')], prompt_input_dict[('channel-12','media')])\n",
    "\n",
    "df_conv_dict[('channel-13','geography')] = run_GK_inference(df_conv_dict[('channel-13','geography')], prompt_input_dict[('channel-13','geography')])\n",
    "df_conv_dict[('channel-13','nutrition')] = run_GK_inference(df_conv_dict[('channel-13','nutrition')], prompt_input_dict[('channel-13','nutrition')])\n",
    "\n",
    "df_conv_dict[('channel-14','sports')] = run_GK_inference(df_conv_dict[('channel-14','sports')], prompt_input_dict[('channel-14','sports')])\n",
    "df_conv_dict[('channel-14','nutrition')] = run_GK_inference(df_conv_dict[('channel-14','nutrition')], prompt_input_dict[('channel-14','nutrition')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6872bbe",
   "metadata": {},
   "source": [
    "## Analysis CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cfd0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_list = ['CLS_zero_n1_llama3_8b', 'CLS_zero_n3_llama3_8b', 'CLS_zero_all_llama3_8b',\n",
    "                 'CLS_few_n1_llama3_8b', 'CLS_few_n3_llama3_8b', 'CLS_few_all_llama3_8b',\n",
    "                 \n",
    "                 'CLS_zero_n1_llama3_70b', 'CLS_zero_n3_llama3_70b', 'CLS_zero_all_llama3_70b',\n",
    "                 'CLS_few_n1_llama3_70b', 'CLS_few_n3_llama3_70b', 'CLS_few_all_llama3_70b',\n",
    "                 \n",
    "                 'CLS_zero_n1_gpt35', 'CLS_zero_n3_gpt35', 'CLS_zero_all_gpt35',\n",
    "                 'CLS_few_n1_gpt35', 'CLS_few_n3_gpt35', 'CLS_few_all_gpt35',\n",
    "                 \n",
    "                 'CLS_zero_n1_gpt4o', 'CLS_zero_n3_gpt4o', 'CLS_zero_all_gpt4o',\n",
    "                 'CLS_few_n1_gpt4o', 'CLS_few_n3_gpt4o', 'CLS_few_all_gpt4o',\n",
    "                ]\n",
    "folder_name = 'clsruns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, label_column=\"grounding_label\", prediction_column=\"CLS_prediction_pp\", avg_m=\"macro\"):\n",
    "    # Extract label and prediction values from the dataframe\n",
    "    CLS_true = df[label_column].replace(\"-\", np.nan).dropna()\n",
    "    # FIX WRONG OUTPUT FORMAT PREPROCESSING\n",
    "    CLS_pred = df[prediction_column].replace(\"\", np.nan).dropna()\n",
    "    unique_values = CLS_pred.unique()\n",
    "    if set(unique_values) != {'implicit', 'explicit', 'clarification'}:\n",
    "        print(f\"Warning: Column {prediction_column} contains invalid or incomplete values: {set(unique_values)}\")\n",
    "    elif (len(CLS_pred) == len(CLS_pred)) and (set(unique_values) == {'implicit', 'explicit', 'clarification'}):\n",
    "        print(f\"Labels OK.\", set(unique_values))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(CLS_true, CLS_pred)\n",
    "    # Calculate precision\n",
    "    precision = precision_score(CLS_true, CLS_pred, average=avg_m, zero_division=0)\n",
    "    # Calculate recall\n",
    "    recall = recall_score(CLS_true, CLS_pred, average=avg_m, zero_division=0)\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(CLS_true, CLS_pred, average=avg_m, zero_division=0)\n",
    "    \n",
    "    print(\"<\", run, \">\", \"Metric Avg:\", avg_m)\n",
    "    print(\"Accuracy:\", round(accuracy,2))\n",
    "    print(\"Precision:\", round(precision,2))\n",
    "    print(\"Recall:\", round(recall,2))\n",
    "    print(\"F1 Score:\", round(f1,2))\n",
    "    print(\"\")\n",
    "\n",
    "for run in run_name_list:\n",
    "    folder_name = 'clsruns'\n",
    "    file_path = folder_name + '/' +  run +'.xlsx'\n",
    "    df = pd.read_excel(file_path, header=0)\n",
    "    \n",
    "    # Define the regex CLS extraction pattern\n",
    "    pattern = r'(explicit|implicit|clarification)'\n",
    "\n",
    "    # Extract the first occurrence of the label and keep the rest of the content after it\n",
    "    df['CLS_prediction_pp'] = \"\"\n",
    "    df['CLS_prediction_pp'] = df['CLS_prediction'].str.extract(pattern, expand=False)\n",
    "    \n",
    "    calculate_metrics(df=df, label_column=\"grounding_label\", prediction_column=\"CLS_prediction_pp\", avg_m=\"macro\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed calculation\n",
    "class_names = ['clarification', 'explicit', 'implicit']\n",
    "\n",
    "# Extract label and prediction values from the dataframe\n",
    "CLS_true = df[\"grounding_label\"].replace(\"-\", np.nan).dropna()\n",
    "# FIX WRONG OUTPUT FORMAT PREPROCESSING\n",
    "CLS_pred = df[\"CLS_prediction\"].str.replace(\"Output Label: \", \"\").str.strip()\n",
    "CLS_pred = CLS_pred.replace(\"\", np.nan).dropna()\n",
    "    \n",
    "# Generate classification report\n",
    "report = classification_report(CLS_true, CLS_pred, target_names=class_names, zero_division=0)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'clsruns'\n",
    "file_path = folder_name + '/' +  'CLS_zero_all_llama3_70b' +'.xlsx'\n",
    "df = pd.read_excel(file_path, header=0)\n",
    "\n",
    "# Define the regex CLS extraction pattern\n",
    "pattern = r'(explicit|implicit|clarification)'\n",
    "\n",
    "# Extract the first occurrence of the label and keep the rest of the content after it\n",
    "df['CLS_prediction_pp'] = \"\"\n",
    "df['CLS_prediction_pp'] = df['CLS_prediction'].str.extract(pattern, expand=False)\n",
    "set(df['CLS_prediction_pp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342fbf9",
   "metadata": {},
   "source": [
    "## Analysis GK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_list = ['GK_zero_shot_llama3_8b', 'GK_few_shot_llama3_8b',\n",
    "                 \n",
    "                 'GK_zero_shot_llama3_70b', 'GK_few_shot_llama3_70b',\n",
    "                 \n",
    "                 'GK_zero_shot_gpt35', 'GK_few_shot_gpt35',\n",
    "                 \n",
    "                 'GK_zero_shot_gpt4o', 'GK_few_shot_gpt4o',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_predictions(df, column):\n",
    "    replacements = [\n",
    "    (\"Output JSON-LD:\", \"\"), \n",
    "    (\"```json-ld\", \"\"),\n",
    "    (\"```json\", \"\"),\n",
    "    (\"```\", \"\")]\n",
    "    \n",
    "    for old, new in replacements:\n",
    "        df[column] = df[column].str.replace(old, new, regex=False).str.strip()\n",
    "\n",
    "def validate_json_ld(df, prompt_input_dict):\n",
    "    def is_valid_json(json_str):\n",
    "        try:\n",
    "            json.loads(json_str)\n",
    "            return 1\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    def get_all_keys(data, keys=None):\n",
    "        if keys is None:\n",
    "            keys = set()\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                keys.add(key)\n",
    "                get_all_keys(value, keys)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                get_all_keys(item, keys)\n",
    "        return keys\n",
    "\n",
    "    def has_expected_schema(json_str, schema_keys):\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            data_keys = get_all_keys(data)\n",
    "            return int(data_keys.issubset(schema_keys))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        \n",
    "    def has_equal_schema(json_str, schema_keys):\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            data_keys = get_all_keys(data)\n",
    "            return int(data_keys == schema_keys)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "            \n",
    "    def get_all_values(data, values=None):\n",
    "        if values is None:\n",
    "            values = set()\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            for value in data.values():\n",
    "                get_all_values(value, values)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                get_all_values(item, values)\n",
    "        else:\n",
    "            if isinstance(data, (str, int, float, bool, type(None))):\n",
    "                values.add(data)\n",
    "        return values\n",
    "\n",
    "    def has_expected_values(json_str, expected_values):\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            values = get_all_values(data)\n",
    "            return int(values.issubset(expected_values))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        \n",
    "    def has_equal_values(json_str, expected_values):\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            values = get_all_values(data)\n",
    "            return int(values == expected_values)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    \n",
    "    def set_difference(json_str, expected_set, set_type=None):\n",
    "        if set_type == \"keys\":\n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "                predicted_set = get_all_keys(data)\n",
    "                difference = predicted_set.difference(expected_set)\n",
    "                return difference\n",
    "            except ValueError:\n",
    "                raise ValueError(\"Invalid JSON input\")\n",
    "        elif set_type == \"values\":\n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "                predicted_set = get_all_values(data)\n",
    "                difference = predicted_set.difference(expected_set)\n",
    "                return difference\n",
    "            except ValueError:\n",
    "                raise ValueError(\"Invalid JSON input\")\n",
    "    \n",
    "    def compare_json_ld(json1, json2):\n",
    "        try:\n",
    "            data1 = json.loads(json1)\n",
    "            data2 = json.loads(json2)\n",
    "            return int(data1 == data2)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    def get_json_diff(json1, json2):\n",
    "        try:\n",
    "            data1 = json.loads(json1)\n",
    "            data2 = json.loads(json2)\n",
    "            diff = DeepDiff(data1, data2, ignore_order=True)\n",
    "            return diff if diff else None\n",
    "        except ValueError:\n",
    "            return \"Invalid JSON\"\n",
    "    \n",
    "    # Initialize new columns with default values\n",
    "    df['GK_valid'] = \"\"\n",
    "    \n",
    "    df['GK_PRED_valid'] = \"\"\n",
    "    df['GK_schema_valid'] = \"\"\n",
    "    df['GK_PRED_schema_valid'] = \"\"\n",
    "    df['GK_PRED_schema_equal'] = \"\"\n",
    "    df['GK_full_schema_valid'] = \"\"\n",
    "    df['GK_PRED_full_schema_valid'] = \"\"\n",
    "    df['GK_PRED_full_schema_diff'] = \"\"\n",
    "    \n",
    "    df['GK_values_valid'] = \"\"\n",
    "    df['GK_PRED_values_valid'] = \"\"\n",
    "    df['GK_PRED_values_equal'] = \"\"\n",
    "    df['GK_all_values_valid'] = \"\"\n",
    "    df['GK_PRED_all_values_valid'] = \"\"\n",
    "    df['GK_PRED_all_values_diff'] = \"\"\n",
    "    \n",
    "    df['content_equal'] = \"\"\n",
    "    df['json_diff'] = \"\"\n",
    "\n",
    "    # Iterate over rows and perform checks\n",
    "    for idx, row in df.iterrows():\n",
    "        if (pd.notna(row['grounded_knowledge']) and pd.notna(row['GK_prediction']) and row['grounded_knowledge'] != \"-\"):\n",
    "            grounded_knowledge = row['grounded_knowledge']\n",
    "            GK_prediction = row['GK_prediction']\n",
    "\n",
    "            # Syntactic validation\n",
    "            gk_valid = is_valid_json(grounded_knowledge)\n",
    "            gk_PRED_valid = is_valid_json(GK_prediction)\n",
    "            df.at[idx, 'GK_valid'] = gk_valid\n",
    "            df.at[idx, 'GK_PRED_valid'] = gk_PRED_valid\n",
    "\n",
    "            if gk_valid and gk_PRED_valid:\n",
    "                # Get schema and full schema\n",
    "                conv_key = (row[\"room_name\"], row[\"topic\"])\n",
    "                #print(\"conv_key:\", conv_key, \"index:\", idx)\n",
    "                #print(\"# SCHEMA\")\n",
    "                system_knowledge = prompt_input_dict[conv_key][0][\"system_knowledge\"]\n",
    "                full_schema_keys = get_all_keys(json.loads(system_knowledge))\n",
    "                schema_keys = get_all_keys(json.loads(grounded_knowledge))\n",
    "                PRED_keys = get_all_keys(json.loads(GK_prediction))\n",
    "                #print(\"full_schema_keys:\", len(full_schema_keys), full_schema_keys)\n",
    "                #print(\"PRED_keys:\", len(PRED_keys), PRED_keys)\n",
    "                \n",
    "                # Schema validation\n",
    "                gk_schema_valid = has_expected_schema(grounded_knowledge, schema_keys)\n",
    "                gk_PRED_schema_valid = has_expected_schema(GK_prediction, schema_keys)\n",
    "                gk_PRED_schema_equal = has_equal_schema(GK_prediction, schema_keys)\n",
    "                df.at[idx, 'GK_schema_valid'] = gk_schema_valid\n",
    "                df.at[idx, 'GK_PRED_schema_valid'] = gk_PRED_schema_valid\n",
    "                df.at[idx, 'GK_PRED_schema_equal'] = gk_PRED_schema_equal\n",
    "                #print(\"schema_equal:\", gk_PRED_schema_equal)\n",
    "                \n",
    "                # Full schema validation\n",
    "                gk_full_schema_valid = has_expected_schema(grounded_knowledge, full_schema_keys)\n",
    "                gk_PRED_full_schema_valid = has_expected_schema(GK_prediction, full_schema_keys)\n",
    "                gk_PRED_full_schema_diff = set_difference(GK_prediction, full_schema_keys, set_type=\"keys\")\n",
    "                #print(\"full_schema_diff:\", len(gk_PRED_full_schema_diff), gk_PRED_full_schema_diff)\n",
    "                df.at[idx, 'GK_full_schema_valid'] = gk_full_schema_valid\n",
    "                df.at[idx, 'GK_PRED_full_schema_valid'] = gk_PRED_full_schema_valid\n",
    "                df.at[idx, 'GK_PRED_full_schema_diff'] = gk_PRED_full_schema_diff\n",
    "\n",
    "                #if gk_full_schema_valid and gk_PRED_full_schema_valid:\n",
    "                    #print(\"# VALUES\")\n",
    "                    # Get values and all values\n",
    "                conv_key = (row[\"room_name\"], row[\"topic\"])\n",
    "                system_knowledge = prompt_input_dict[conv_key][0][\"system_knowledge\"]\n",
    "                all_values = get_all_values(json.loads(system_knowledge))\n",
    "                values = get_all_values(json.loads(grounded_knowledge))\n",
    "                PRED_values = get_all_values(json.loads(GK_prediction))\n",
    "                #print(\"all_values:\", len(all_values), all_values)\n",
    "                #print(\"values:\", len(values), values)\n",
    "                #print(\"PRED_values:\", len(PRED_values), PRED_values)\n",
    "\n",
    "                # Values validation\n",
    "                gk_values_valid = has_expected_values(grounded_knowledge, values)\n",
    "                gk_PRED_values_valid = has_expected_values(GK_prediction, values)\n",
    "                gk_PRED_values_equal = has_equal_values(GK_prediction, values)\n",
    "                df.at[idx, 'GK_values_valid'] = gk_values_valid\n",
    "                df.at[idx, 'GK_PRED_values_valid'] = gk_PRED_values_valid\n",
    "                df.at[idx, 'GK_PRED_values_equal'] = gk_PRED_values_equal\n",
    "                #print(\"values_equal:\", gk_PRED_schema_equal)\n",
    "\n",
    "                # All values validation\n",
    "                gk_all_values_valid = has_expected_values(grounded_knowledge, all_values)\n",
    "                gk_PRED_all_values_valid = has_expected_values(GK_prediction, all_values)\n",
    "                gk_PRED_all_values_diff = set_difference(GK_prediction, all_values, set_type=\"values\")\n",
    "                #print(\"all_values_diff:\", len(gk_PRED_all_values_diff), gk_PRED_all_values_diff)\n",
    "                df.at[idx, 'GK_all_values_valid'] = gk_all_values_valid\n",
    "                df.at[idx, 'GK_PRED_all_values_valid'] = gk_PRED_all_values_valid\n",
    "                df.at[idx, 'GK_PRED_all_values_diff'] = gk_PRED_all_values_diff\n",
    "                    \n",
    "                    \n",
    "                    #if gk_all_values_valid and gk_PRED_all_values_valid:\n",
    "                    \n",
    "                # Content validation\n",
    "                content_equal = compare_json_ld(grounded_knowledge, GK_prediction)\n",
    "                df.at[idx, 'content_equal'] = content_equal\n",
    "\n",
    "                if not content_equal:\n",
    "                    # Detailed content differences\n",
    "                    json_diff = get_json_diff(grounded_knowledge, GK_prediction)\n",
    "                    df.at[idx, 'json_diff'] = json_diff\n",
    "                #print(\"\")\n",
    "        else: continue\n",
    "\n",
    "    return df\n",
    "        \n",
    "\n",
    "def calculate_accuracy_metrics(df, run_name):\n",
    "    # List of columns to calculate metrics for\n",
    "    columns = ['GK_PRED_valid', 'GK_PRED_full_schema_valid', 'GK_PRED_schema_valid', 'GK_PRED_schema_equal', 'GK_PRED_all_values_valid', 'GK_PRED_values_valid', 'GK_PRED_values_equal', 'content_equal']\n",
    "    \n",
    "    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Dictionary to store the count (accuracy) of each column\n",
    "    metrics = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            metrics[col] = df[col].sum()\n",
    "        else:\n",
    "            metrics[col] = None\n",
    "            \n",
    "    valid_predictions = metrics[\"GK_PRED_valid\"]\n",
    "    \n",
    "    metrics['GK_PRED_full_schema_diff'] = df['GK_PRED_full_schema_diff'].apply(lambda x: isinstance(x, set) and len(x) > 0).sum()\n",
    "    metrics['GK_PRED_all_values_diff'] = df['GK_PRED_all_values_diff'].apply(lambda x: isinstance(x, set) and len(x) > 0).sum()\n",
    "\n",
    "    \n",
    "    # Print formatted metrics\n",
    "    print(\"<\", run, \">\")\n",
    "    for col, mean in metrics.items():\n",
    "        if mean is not None:\n",
    "            print(f\"{col}: {round(mean,2)}\")\n",
    "        else:\n",
    "            print(f\"{col}: Column not found in DataFrame.\")\n",
    "    print('1 - GK_PRED_valid %',  round(1-metrics['GK_PRED_valid']/127,2))\n",
    "    print('GK_PRED_full_schema_diff %',  round(metrics['GK_PRED_full_schema_diff']/valid_predictions,2))\n",
    "    print('GK_PRED_all_values_diff %',  round(metrics['GK_PRED_all_values_diff']/valid_predictions,2))\n",
    "    print(\"\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def extract_json_ld(text):\n",
    "    # Check if the text is valid JSON\n",
    "    if isinstance(text, (float, type(None))):  # Check for NaN and None\n",
    "        return None\n",
    "    text = str(text)\n",
    "    \n",
    "    try:\n",
    "        json.loads(text)\n",
    "        return text\n",
    "    except ValueError:\n",
    "        # If not valid JSON, try to extract with regex\n",
    "        pattern = r'(?:(?<=\\n)|(?<=^))\\[\\s*.*?\\s*\\](?:(?=\\n)|(?=$))'\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in run_name_list:\n",
    "    folder_name = 'gkruns'\n",
    "    file_path = folder_name + '/' +  run +'.xlsx'\n",
    "    df = pd.read_excel(file_path, header=0)\n",
    "    preprocess_predictions(df, column=\"GK_prediction\")\n",
    "    df['GK_prediction'] = df['GK_prediction'].map(lambda x: extract_json_ld(x))\n",
    "    df = validate_json_ld(df, prompt_input_dict)\n",
    "    df.to_excel('evaluation/' + 'eval_' + run + \".xlsx\", index=False)\n",
    "    \n",
    "    calculate_accuracy_metrics(df=df, run_name=run) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cec07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values can be valid but schema not valid, e.g. messes up primary key and only includes a subset of values not all, it can also associate wrong values with wrong entities butter 0 calories\n",
    "# use value_counts to calculate [0][1] using valid\n",
    "GK_PRED_values_equal = 17 + 2 # just the order or number can be wrong (one item removed)\n",
    "GK_PRED_values_valid = 19 + 29 # values not complete vocabulary because e.g. underpedicted with no category, oder not there exists an instance of the year/category ... E\n",
    "GK_PRED_schema_equal = 35\n",
    "GK_PRED_schema_valid = 35 + 30 # keys not complute vocabulary, sometimes forgets category for instances (tuna seafood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cols =  ['GK_PRED_schema_equal', 'GK_PRED_values_equal', 'content_equal']\n",
    "\n",
    "df_1 = pd.read_excel('evaluation/eval_GK_few_shot_llama3_8b.xlsx', header=0)\n",
    "df_1 = df_1[filter_cols]\n",
    "\n",
    "\n",
    "value_counts_1 = df_1.apply(pd.Series.value_counts).reindex([1, 0]).fillna(0).loc[[1, 0]].T\n",
    "value_counts_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dict_keys(df, column):\n",
    "    \"\"\"\n",
    "    Count the occurrences of dictionary keys in a specified DataFrame column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the column to check.\n",
    "    column (str): The name of the column to check for dictionaries.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with keys and their corresponding counts.\n",
    "    \"\"\"\n",
    "    key_counter = Counter()\n",
    "\n",
    "    for item in df[column]:\n",
    "        try:\n",
    "            # Convert the item to a dictionary if it's a valid dictionary string\n",
    "            item_dict = ast.literal_eval(item) if isinstance(item, str) else item\n",
    "            if isinstance(item_dict, dict):\n",
    "                key_counter.update(item_dict.keys())\n",
    "        except (ValueError, SyntaxError):\n",
    "            # Skip items that are not dictionaries\n",
    "            continue\n",
    "\n",
    "    return dict(key_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67846aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_list = ['GK_zero_shot_llama3_8b', 'GK_few_shot_llama3_8b',\n",
    "                 \n",
    "                 'GK_zero_shot_llama3_70b', 'GK_few_shot_llama3_70b',\n",
    "                 \n",
    "                 'GK_zero_shot_gpt35', 'GK_few_shot_gpt35',\n",
    "                 \n",
    "                 'GK_zero_shot_gpt4o', 'GK_few_shot_gpt4o',\n",
    "                ]\n",
    "\n",
    "change_keys = ['iterable_item_added', 'dictionary_item_added', 'iterable_item_removed', 'dictionary_item_removed', 'values_changed']\n",
    "\n",
    "def calculate_error_metrics(run_name):\n",
    "    folder_name = 'gkruns'\n",
    "    file_path = folder_name + '/' + run_name +'.xlsx'\n",
    "    df = pd.read_excel(file_path, header=0)\n",
    "    preprocess_predictions(df, column=\"GK_prediction\")\n",
    "    df['GK_prediction'] = df['GK_prediction'].map(lambda x: extract_json_ld(x))\n",
    "    df = validate_json_ld(df, prompt_input_dict)  \n",
    "\n",
    "    df_sum_valid_json = len(df[df[\"GK_PRED_valid\"]==1])\n",
    "    df_sum_difflib = len(df[df[\"content_equal\"]==0])\n",
    "    df_changes = count_dict_keys(df, \"json_diff\")\n",
    "    df_sum_changes = sum(df_changes.values())\n",
    "    print(\"<\", run_name, \">\")\n",
    "    for key in change_keys:\n",
    "        print(key, df_changes[key], round(df_changes[key]/df_sum_changes, 2))\n",
    "    sum_schema = len(df[df[\"GK_PRED_schema_equal\"].isin([0, 1])])\n",
    "    sum_schema_equal = len(df[df[\"GK_PRED_schema_equal\"]==1])\n",
    "    sum_schema_less = len(df[df[\"GK_PRED_schema_valid\"]==1]) - sum_schema_equal\n",
    "    sum_schema_more = sum_schema - sum_schema_equal - sum_schema_less\n",
    "    print(\"sum_valid_json\", df_sum_valid_json)\n",
    "    print(\"schema_equal\", sum_schema_equal, round(sum_schema_equal/sum_schema, 2))\n",
    "    print(\"schema_more\", sum_schema_more, round(sum_schema_more/sum_schema, 2))\n",
    "    print(\"schema_less\", sum_schema_less, round(sum_schema_less/sum_schema, 2))\n",
    "    print(\"sum_schema\", sum([sum_schema_equal, sum_schema_more, sum_schema_less]))\n",
    "    sum_values = len(df[df[\"GK_PRED_values_equal\"].isin([0, 1])])\n",
    "    sum_values_equal = len(df[df[\"GK_PRED_values_equal\"]==1])\n",
    "    sum_values_less = len(df[df[\"GK_PRED_values_valid\"]==1]) - sum_values_equal\n",
    "    sum_values_more = sum_values - sum_values_equal - sum_values_less\n",
    "    print(\"values_equal\", sum_values_equal, round(sum_values_equal/sum_values, 2))\n",
    "    print(\"values_more\", sum_values_more, round(sum_values_more/sum_values, 2))\n",
    "    print(\"values_less\", sum_values_less, round(sum_values_less/sum_values, 2))\n",
    "    print(\"sum_values\", sum([sum_values_equal, sum_values_more, sum_values_less]))\n",
    "    print(\"\\n\")\n",
    "    return df\n",
    "\n",
    "for run_name in run_name_list:\n",
    "    calculate_error_metrics(run_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
